{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data - Tweets From A World Leader\n",
    "\n",
    "### Data collection\n",
    "This sample data consists of tweets of Donald Trump for the year 2019 (before he was permanently banned from twitter (X) and when was the President). The main problem statement is - can the tweets and the sentiment of the tweets predict behaviour at individual, organizational or government level ?   \n",
    "\n",
    "At an individual level, questions could deal with some antecedents that predict the sentiment of the tweets. In this case, we could check if the day/ month/ time can predict the mood of an individual and the sentiment of the tweet. At an organizational level, we could think of whether the language of the tweets can predict the performance of an organization. Here, we could try to predict changes to the monthly sales/ revenues given the nature of the tweets. At a government level, we could check if the monthly agregate nature of tweets could predict certain country-level macro indicators. Here, we could study if some macro indicator like monthly jobs added may change with the nature of text in the tweets. \n",
    "\n",
    "**In this example, we try to predict the movement of the NASDAQ given the text of the tweet by a world leader.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Tweets of Trump:\n",
    "The data comes from a web based twitter archive https://www.thetrumparchive.com/faq . This has all the tweets of Donald Trump. From this set I could extract the tweets from the year 2019. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting NASDAQ data:\n",
    "You have many online databases for the daily stock and index prices. The file NASDAQ_Close_2019.csv, included, has the NASDAQ daily index data for 2019. It has been downloaded from -\n",
    "https://finance.yahoo.com/quote/%5EIXIC/history?period1=1546300800&period2=1577836800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation\n",
    "\n",
    "There may be some data tranformation you may need to do to the collected data. In this example, we seperate the day, month, and year of the tweet and store it in seperate columns. You can use Python or MS Excel for this data manipulation. Note: There is a nice module called \"datetime\" specifically designed to manipulate date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation - Merging Tweet Data and Stock Index Data\n",
    "With the tweets ready, the next task is to merge the tweet data with the main predictor variable. In this case we are trying to predict the direction of movement of the NASDAQ index, hence we need to merge daily stock index data with the collected tweet data. You can use MS Excel operations to clean and manipulate data (like seperate the day, month, year).  \n",
    "\n",
    "To merge csv files in Python, I use a module called Pandas, which provides a tabular data structure called Dataframe. This module provides a simple and efficient merge operation. You can also merge using for-loops, but that will be time consuming and inefficient. I merge using the Day, Month and Year infomation of the tweets and the index data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To use the code, assign the file path to the NEW_TWEET_CSV, MERGE_CSV variables.\n",
    "# Assign the path of NASDAQ_Close_2019.csv file to INDEX_CSV variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "NEW_TWEET_CSV = \"https://raw.githubusercontent.com/Minh-Khoa-Pham/DSS_Assignment_2/main/Trump_Tweets_2019.csv?token=GHSAT0AAAAAACKHOFUVB3SR4G6Z5VVFXTGSZKSBJTA\"\n",
    "INDEX_CSV = \"https://raw.githubusercontent.com/Minh-Khoa-Pham/DSS_Assignment_2/main/NASDAQ_Close_2019.csv?token=GHSAT0AAAAAACKHOFUU4ER3L3NHTBQLIE56ZKSBPLQ\"\n",
    "MERGE_CSV = \"https://raw.githubusercontent.com/Minh-Khoa-Pham/DSS_Assignment_2/main/Merged_Tweet_NASDAQ_2019.csv?token=GHSAT0AAAAAACKHOFUVUG3KY6HFMIHXBIKMZKSBPYQ\"\n",
    "\n",
    "# Here i read the csv file and import the data into python. Since the csv has no headers\n",
    "# names = [\"Tweet_date\",\"Tweet_text\",\"Tweet_day\",\"Tweet_month\",\"Tweet_year\"] is used to \n",
    "# assign column names for for the csv data\n",
    "df_tweet = pd.read_csv(NEW_TWEET_CSV, header = None, names= [\"Tweet_date\",\"Tweet_text\",\"Tweet_day\",\"Tweet_month\",\"Tweet_year\"])\n",
    "\n",
    "# The INDEX data csv file has header information at the '0' row\n",
    "df_idx = pd.read_csv(INDEX_CSV, header= 0)\n",
    "\n",
    "# .merge is used to merge the NEW_TWEET_CSV and INDEX_CSV using the date information\n",
    "# left_on specifies the column headers in the tweet csv that should be matched. \n",
    "# right_on specifies the column headers on INDEX_CSV that need to be matched \n",
    "merge_df = pd.merge(df_tweet, df_idx,  how='left', left_on=['Tweet_day','Tweet_month','Tweet_year'], right_on = ['Day','Month','Year'])\n",
    "# here i drop the rows that do not have stock index data. Note: Stock index is closed on\n",
    "# Saturday and Sunday. But tweets keep coming!\n",
    "merge_df= merge_df.dropna(axis=0,subset = ['Direction'])\n",
    "merge_df.to_csv(MERGE_CSV) #Save the data in the MERGE_CSV file\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Tweet_date  \\\n",
      "0  Tue Dec 31 23:35:58 +0000 2019   \n",
      "\n",
      "                                          Tweet_text  Tweet_day  Tweet_month  \\\n",
      "0  RT @WhiteHouse: Americans saw plenty of Washin...         31           12   \n",
      "\n",
      "   Tweet_year        Date         Open         High          Low        Close  \\\n",
      "0        2019  31/12/2019  8918.740234  8975.360352  8912.769531  8972.599609   \n",
      "\n",
      "     Adj Close        Volume  Direction   Day  Month    Year  \n",
      "0  8972.599609  2.182800e+09        1.0  31.0   12.0  2019.0  \n"
     ]
    }
   ],
   "source": [
    "#Check the merged data frame\n",
    "print(merge_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of labelled data  5359\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_data = merge_df\n",
    "#Check the dataframe size\n",
    "print(\"Size of labelled data \", len(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainig data  4287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle #To shuffle the dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_data = shuffle(label_data)\n",
    "# We need to split the labelled data into trining and testing sets. USe 80-20 split for the labelled data.\n",
    "df_train, df_test = train_test_split(label_data, test_size=0.2)\n",
    "print(\"Size of trainig data \", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def feature_extractor(label_text):\n",
    "  \"\"\"This function converts the input sentence into its features\"\"\"\n",
    "  gen_feature = CountVectorizer(strip_accents = \"unicode\", analyzer=\"word\", stop_words=\"english\", ngram_range=(1,2), max_features=10000)\n",
    "  gen_feature.fit(label_text)\n",
    "  return gen_feature\n",
    "\n",
    "label_data['Tweet_text'] = label_data['Tweet_text'].str.lower() #Change all the tweets into lower case\n",
    "label_data['Tweet_text'] = label_data['Tweet_text'].str.replace('[^\\w\\s]', '', regex=True) #Remove all punctuations in the tweets\n",
    "label_data['Tweet_text'] = label_data['Tweet_text'].replace('\\d+', 'NUM', regex=True)  # Replace numbers\n",
    "label_data['Tweet_text'] = label_data['Tweet_text'].replace(r'http\\S+', '', regex=True)  # Remove URLs\n",
    "\n",
    "gen_feature = feature_extractor(label_data['Tweet_text']) \n",
    "train_x = gen_feature.transform(df_train['Tweet_text'])\n",
    "test_x = gen_feature.transform(df_test['Tweet_text'])\n",
    "print(type(train_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of mnb is =  0.6138059701492538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB() #initialize the classifier object\n",
    "classifier = mnb.fit(train_x, df_train['Direction']) #train the model\n",
    "acc = classifier.score(test_x,df_test['Direction']) #check the accuracy score\n",
    "print(\"accuracy of mnb is = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.53      0.50      0.51       441\n",
      "         1.0       0.66      0.70      0.68       631\n",
      "\n",
      "    accuracy                           0.61      1072\n",
      "   macro avg       0.60      0.60      0.60      1072\n",
      "weighted avg       0.61      0.61      0.61      1072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check other metrics of the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(test_x)\n",
    "print(classification_report(df_test['Direction'], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest classifier is = 0.5998134328358209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_model = rf_classifier.fit(train_x, df_train['Direction'])\n",
    "rf_acc = rf_model.score(test_x, df_test['Direction'])\n",
    "print(\"Accuracy of Random Forest classifier is =\", rf_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.52      0.38      0.44       441\n",
      "         1.0       0.64      0.75      0.69       631\n",
      "\n",
      "    accuracy                           0.60      1072\n",
      "   macro avg       0.58      0.57      0.56      1072\n",
      "weighted avg       0.59      0.60      0.59      1072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_y_pred = rf_classifier.predict(test_x)\n",
    "print(classification_report(df_test['Direction'], rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier is = 0.585820895522388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_model = dt_classifier.fit(train_x, df_train['Direction'])\n",
    "\n",
    "dt_acc = dt_model.score(test_x, df_test['Direction'])\n",
    "print(\"Accuracy of Decision Tree classifier is =\", dt_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.50      0.51      0.51       441\n",
      "         1.0       0.65      0.64      0.64       631\n",
      "\n",
      "    accuracy                           0.59      1072\n",
      "   macro avg       0.57      0.58      0.57      1072\n",
      "weighted avg       0.59      0.59      0.59      1072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_y_pred = dt_classifier.predict(test_x)\n",
    "print(classification_report(df_test['Direction'], dt_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "Since the Naive Bayes model has the highest accuracy score (0.61), we will use the Naive Bayes model to predict stock movement based on Trump tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the tweet for analysis I am asking for everyone at the U.S. Capitol to remain peaceful. No violence! Remember, WE are the Party of Law & Order â€“ respect the Law and our great men and women in Blue. Thank you!\n",
      "NASDAQ stock direction:  [1.]\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter the tweet for analysis: \").lower()\n",
    "\n",
    "s_l = pd.Series(s)\n",
    "x = gen_feature.transform(s_l)\n",
    "print('NASDAQ stock direction: ',classifier.predict(x))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
